{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\train_data.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\test_data.csv\")\n",
    "\n",
    "combined = pd.concat([train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excerpts_by_source(df, source):\n",
    "    return df[df['source']==source]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "txt_files = os.listdir(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\CUAD_v1\\full_contract_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = [x for x in txt_files if x.split('.')[-1]=='txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [7:03:10<00:00, 49.79s/it]     \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def word_order_distance(w, excerpt):\n",
    "    w_words = w.split()\n",
    "    excerpt_words = excerpt.split()\n",
    "    match = SequenceMatcher(None, w_words, excerpt_words)\n",
    "    return match.ratio()\n",
    "\n",
    "def excerpt_splitter(text, characters_to_split_on=['\\n\\n','\\n','.']):\n",
    "    excerpts = []\n",
    "    for char in characters_to_split_on:\n",
    "        if char in text:\n",
    "            excerpts += text.split(char)\n",
    "    excerpts = [x for x in excerpts if x != '' and len(x) > 10]\n",
    "    excerpts = [x for x in excerpts if len(x.split()) > 10]\n",
    "    return excerpts\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for file in tqdm(txt_files):\n",
    "\n",
    "    file_pdf = file.replace('.txt', '.pdf')\n",
    "    important_excerpts = get_excerpts_by_source(combined, file_pdf)\n",
    "\n",
    "    # replace any mention of ' (Page n)' with '' where n is any number of up to 3 digits\n",
    "    pattern = re.compile(r' \\(Page \\d{1,3}\\)')\n",
    "    important_excerpts = [re.sub(pattern, '', x) for x in important_excerpts]\n",
    "\n",
    "    # replace any mention of ' (Pages n-n)' with '' where n is any number of up to 3 digits\n",
    "    pattern = re.compile(r' \\(Pages \\d{1,3}-\\d{1,3}\\)')\n",
    "    important_excerpts = [re.sub(pattern, '', x) for x in important_excerpts]\n",
    "\n",
    "    important_excerpts = [x.replace('<omitted> ','').replace('\\n\\n',' ').replace('\\n',' ') for x in important_excerpts]\n",
    "\n",
    "    with open(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\CUAD_v1\\full_contract_txt\\\\\"+file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "\n",
    "    for excerpt in important_excerpts:\n",
    "        w = len(excerpt)\n",
    "        s = 1\n",
    "        windows = []\n",
    "        doc_words = txt.split(' ')\n",
    "        for i in range(0, len(doc_words), s):\n",
    "            k = i + 1\n",
    "            subtext = doc_words[i:k]\n",
    "            while len(' '.join(subtext)) < w:\n",
    "                subtext = doc_words[i:k]\n",
    "                k += 1\n",
    "                if k > len(doc_words):\n",
    "                    break\n",
    "            windows.append(' '.join(subtext))\n",
    "\n",
    "        scores = [word_order_distance(w, excerpt) for w in windows]\n",
    "        score = np.argmax(scores)\n",
    "        max_score = max(scores)\n",
    "        if max_score >= 0.75:\n",
    "            txt = txt.replace(windows[score], '')\n",
    "\n",
    "    pattern = re.compile(r'Source: .*\\n')\n",
    "    txt = re.sub(pattern, '', txt)\n",
    "\n",
    "    excerpts = excerpt_splitter(txt)\n",
    "\n",
    "    negative_data = {'text': excerpts, 'source': [file_pdf]*len(excerpts), 'label':'not_important'}\n",
    "    positive_data = {'text': important_excerpts, 'source': [file_pdf]*len(important_excerpts), 'label':'important'}\n",
    "\n",
    "    negative_df = pd.DataFrame(negative_data)\n",
    "    positive_df = pd.DataFrame(positive_data)\n",
    "\n",
    "    together = pd.concat([negative_df, positive_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    dataset = pd.concat([dataset, together], axis=0).reset_index(drop=True)\n",
    "\n",
    "dataset.to_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\importance_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "not_important    180435\n",
       "important          3440\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "not_important    180435\n",
       "important          3440\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\importance_dataset.csv\")\n",
    "\n",
    "df['label'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6880\n"
     ]
    }
   ],
   "source": [
    "not_important = df[df['label']=='not_important']\n",
    "important = df[df['label']=='important']\n",
    "\n",
    "not_important = not_important.sample(important.shape[0])\n",
    "\n",
    "df = pd.concat([not_important, important], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train.to_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\importance_train.csv\", index=False)\n",
    "test.to_csv(r\"C:\\Users\\paulw\\Documents\\QuantSpark\\semantic_scanner\\data\\importance_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_scanner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
